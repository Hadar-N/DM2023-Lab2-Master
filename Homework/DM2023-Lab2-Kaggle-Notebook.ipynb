{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0d4eaa",
   "metadata": {},
   "source": [
    "Code Introduction:\n",
    "\n",
    "This is the one of the latest versions of the code, but sadly not exacly the same as the one reached the max score (the score was similar through most of my submissions)\n",
    "\n",
    "As the dataset is based off texts, similarily to the dataset used in the lab itself, I used a lot of the same code to create the model.\n",
    "Tried both CountVectorizer and TF/IDF one (ended up using the cound, as it got better results), also tried different sizes of BagOfWords/tokenizers, also different models (DFTree/Multi_nb/Keras etc) and layer sizes. This is the final configulation attemped with this specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cf5a2c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-29T10:38:22.655735Z",
     "iopub.status.busy": "2023-12-29T10:38:22.655365Z",
     "iopub.status.idle": "2023-12-29T10:38:37.396209Z",
     "shell.execute_reply": "2023-12-29T10:38:37.395110Z"
    },
    "papermill": {
     "duration": 14.750954,
     "end_time": "2023-12-29T10:38:37.398457",
     "exception": false,
     "start_time": "2023-12-29T10:38:22.647503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cleantext\r\n",
      "  Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from cleantext) (3.2.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->cleantext) (1.16.0)\r\n",
      "Installing collected packages: cleantext\r\n",
      "Successfully installed cleantext-1.1.4\r\n",
      "/kaggle/input/dm-lab2/tweets_DM.json\n",
      "/kaggle/input/dm-lab2/sampleSubmission.csv\n",
      "/kaggle/input/dm-lab2/data_identification.csv\n",
      "/kaggle/input/dm-lab2/emotion.csv\n"
     ]
    }
   ],
   "source": [
    "# Default code importing Kaggle files + getting the paths to the files\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Added this package to clean the text\n",
    "!pip install cleantext\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f746d001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:38:37.413666Z",
     "iopub.status.busy": "2023-12-29T10:38:37.413095Z",
     "iopub.status.idle": "2023-12-29T10:39:50.226109Z",
     "shell.execute_reply": "2023-12-29T10:39:50.225159Z"
    },
    "papermill": {
     "duration": 72.828237,
     "end_time": "2023-12-29T10:39:50.233630",
     "exception": false,
     "start_time": "2023-12-29T10:38:37.405393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape (1867535, 5)\n",
      "data_division.shape (1867535, 2)\n",
      "data_emotions.shape (1455563, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1-\n",
    "\n",
    "# imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import keras\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from cleantext import clean\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#imports + readFiles\n",
    "\n",
    "KAGGLE_JSON = '/kaggle/input/dm-lab2/tweets_DM.json'\n",
    "KAGGLE_DIVIDE = '/kaggle/input/dm-lab2/data_identification.csv'\n",
    "KAGGLE_EMOTIONS = '/kaggle/input/dm-lab2/emotion.csv'\n",
    "\n",
    "LOGS_FILE = '/kaggle/working/training_log.csv'\n",
    " \n",
    "data_from_json = pd.read_json(KAGGLE_JSON, lines=True, orient='records')\n",
    "\n",
    "data_division = pd.read_csv(KAGGLE_DIVIDE)\n",
    "data_emotions = pd.read_csv(KAGGLE_EMOTIONS)\n",
    "\n",
    "print(\"data.shape\", data_from_json.shape)\n",
    "print(\"data_division.shape\", data_division.shape)\n",
    "print(\"data_emotions.shape\", data_emotions.shape)\n",
    "\n",
    "data_from_json.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d75ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:39:50.250013Z",
     "iopub.status.busy": "2023-12-29T10:39:50.248916Z",
     "iopub.status.idle": "2023-12-29T10:40:23.647838Z",
     "shell.execute_reply": "2023-12-29T10:40:23.646738Z"
    },
    "papermill": {
     "duration": 33.415314,
     "end_time": "2023-12-29T10:40:23.655938",
     "exception": false,
     "start_time": "2023-12-29T10:39:50.240624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>People who post \"add me on Snapchat\" must be d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2  Confident of your obedience, I write to you, k...   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "4  \"Trust is not the same as faith. A friend is s...   \n",
       "\n",
       "                                       text_combined  \n",
       "0  People who post \"add me on Snapchat\" must be d...  \n",
       "1  @brianklaas As we see, Trump is dangerous to f...  \n",
       "2  Confident of your obedience, I write to you, k...  \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
       "4  \"Trust is not the same as faith. A friend is s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating different DataFrame to hold the text itself from the lower level in the dict \n",
    "data_internal = pd.DataFrame(data_from_json[\"_source\"].apply(lambda x: x[\"tweet\"]).to_list())\n",
    "\n",
    "# cleaning stage 1- replace all of the hashtags with the actual words (this way the hashtag sign won't intervene with the text analysis)\n",
    "def replace_all(text, hasht):\n",
    "    for h in hasht:\n",
    "        text = text.replace(f'#{h}', h)\n",
    "    return text\n",
    "data_internal[\"text_combined\"] = data_internal.apply(lambda x: replace_all(x[\"text\"], x[\"hashtags\"]), axis=1)\n",
    "data_internal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3abbb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:40:23.673257Z",
     "iopub.status.busy": "2023-12-29T10:40:23.672293Z",
     "iopub.status.idle": "2023-12-29T10:49:09.146653Z",
     "shell.execute_reply": "2023-12-29T10:49:09.145704Z"
    },
    "papermill": {
     "duration": 525.493114,
     "end_time": "2023-12-29T10:49:09.156869",
     "exception": false,
     "start_time": "2023-12-29T10:40:23.663755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished clean time: 525.4585771560669\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>people who post add me on snapchat must be deh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>brianklaas as we see trump is dangerous to fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>confident of your obedience i write to you kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>now issa is stalking tasha 😂😂😂 lh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>trust is not the same as faith a friend is som...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  tweet_id  \\\n",
       "0                     [Snapchat]  0x376b20   \n",
       "1  [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "2                   [bibleverse]  0x28b412   \n",
       "3                             []  0x1cd5b0   \n",
       "4                             []  0x2de201   \n",
       "\n",
       "                                                text  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2  Confident of your obedience, I write to you, k...   \n",
       "3                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "4  \"Trust is not the same as faith. A friend is s...   \n",
       "\n",
       "                                       text_combined  \n",
       "0  people who post add me on snapchat must be deh...  \n",
       "1  brianklaas as we see trump is dangerous to fre...  \n",
       "2  confident of your obedience i write to you kno...  \n",
       "3                  now issa is stalking tasha 😂😂😂 lh  \n",
       "4  trust is not the same as faith a friend is som...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# cleaning stage 2- remove other special signs/sunctuations/numbers and lowercase the text as a whole to have a unified structure\n",
    "data_internal['text_combined'] = data_internal['text_combined'].apply(lambda x: clean(x, lowercase=True, numbers=True, punct=True))\n",
    "print(\"finished clean time:\", time.time() - start_time)\n",
    "data_internal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadf8262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:49:09.173344Z",
     "iopub.status.busy": "2023-12-29T10:49:09.172520Z",
     "iopub.status.idle": "2023-12-29T10:49:19.112664Z",
     "shell.execute_reply": "2023-12-29T10:49:19.111641Z"
    },
    "papermill": {
     "duration": 9.951205,
     "end_time": "2023-12-29T10:49:19.115314",
     "exception": false,
     "start_time": "2023-12-29T10:49:09.164109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>internal_id</th>\n",
       "      <th>division_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [internal_id, division_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if the tweet_ids order don't match between the different files. if this is really the case, then set all ids to be tweet_id\n",
    "check_orders=pd.DataFrame({\"internal_id\": data_internal[\"tweet_id\"], \"division_id\": data_division[\"tweet_id\"]})\n",
    "if len(check_orders[check_orders[\"internal_id\"] != check_orders[\"division_id\"]]) > 0:\n",
    "    data_internal.set_index('tweet_id', inplace = True, drop = False)\n",
    "    data_division.set_index('tweet_id', inplace = True, drop = False)\n",
    "    data_emotions.set_index('tweet_id', inplace = True, drop = False)\n",
    "    \n",
    "#check again if ids match (should be empty)\n",
    "check_orders2=pd.DataFrame({\"internal_id\": data_internal[\"tweet_id\"], \"division_id\": data_division[\"tweet_id\"]})\n",
    "check_orders2[check_orders2[\"internal_id\"] != check_orders2[\"division_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627fe4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:49:19.169521Z",
     "iopub.status.busy": "2023-12-29T10:49:19.169013Z",
     "iopub.status.idle": "2023-12-29T10:49:22.845144Z",
     "shell.execute_reply": "2023-12-29T10:49:22.843224Z"
    },
    "papermill": {
     "duration": 3.687663,
     "end_time": "2023-12-29T10:49:22.847194",
     "exception": false,
     "start_time": "2023-12-29T10:49:19.159531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_division.shape (1867535, 2)\n",
      "data_emotions.shape (1455563, 2)\n",
      "all emotions merged: ['anticipation' 'sadness' 'fear' 'joy' 'anger' 'trust' 'disgust'\n",
      " 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# divide the data into train and test (which has no knwon emotions) then add the emotions to the train set\n",
    "print(\"data_division.shape\", data_division.shape)\n",
    "\n",
    "data_internal[\"class\"] = data_division[\"identification\"]\n",
    "\n",
    "print(\"data_emotions.shape\", data_emotions.shape)\n",
    "test_lines = data_internal[data_internal[\"class\"] == \"test\"]\n",
    "train_lines = data_internal[data_internal[\"class\"] == \"train\"]\n",
    "\n",
    "train_lines[\"emotion\"] = data_emotions[\"emotion\"]\n",
    "\n",
    "print(\"all emotions merged:\", train_lines[\"emotion\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6743d4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:49:22.888206Z",
     "iopub.status.busy": "2023-12-29T10:49:22.887914Z",
     "iopub.status.idle": "2023-12-29T10:49:24.355763Z",
     "shell.execute_reply": "2023-12-29T10:49:24.354761Z"
    },
    "papermill": {
     "duration": 1.478804,
     "end_time": "2023-12-29T10:49:24.357870",
     "exception": false,
     "start_time": "2023-12-29T10:49:22.879066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>ratio</th>\n",
       "      <th>train_ratio</th>\n",
       "      <th>test_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>412724</td>\n",
       "      <td>103293</td>\n",
       "      <td>0.250271</td>\n",
       "      <td>0.354437</td>\n",
       "      <td>0.354821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>199016</td>\n",
       "      <td>49919</td>\n",
       "      <td>0.250829</td>\n",
       "      <td>0.170910</td>\n",
       "      <td>0.171476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>164429</td>\n",
       "      <td>41049</td>\n",
       "      <td>0.249646</td>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.141007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>154629</td>\n",
       "      <td>38808</td>\n",
       "      <td>0.250975</td>\n",
       "      <td>0.132791</td>\n",
       "      <td>0.133309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>111538</td>\n",
       "      <td>27563</td>\n",
       "      <td>0.247118</td>\n",
       "      <td>0.095786</td>\n",
       "      <td>0.094681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>51112</td>\n",
       "      <td>12887</td>\n",
       "      <td>0.252133</td>\n",
       "      <td>0.043894</td>\n",
       "      <td>0.044268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>39057</td>\n",
       "      <td>9672</td>\n",
       "      <td>0.247638</td>\n",
       "      <td>0.033541</td>\n",
       "      <td>0.033224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>31945</td>\n",
       "      <td>7922</td>\n",
       "      <td>0.247989</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>0.027213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train    test     ratio  train_ratio  test_ratio\n",
       "emotion                                                        \n",
       "joy           412724  103293  0.250271     0.354437    0.354821\n",
       "anticipation  199016   49919  0.250829     0.170910    0.171476\n",
       "trust         164429   41049  0.249646     0.141207    0.141007\n",
       "sadness       154629   38808  0.250975     0.132791    0.133309\n",
       "disgust       111538   27563  0.247118     0.095786    0.094681\n",
       "fear           51112   12887  0.252133     0.043894    0.044268\n",
       "surprise       39057    9672  0.247638     0.033541    0.033224\n",
       "anger          31945    7922  0.247989     0.027434    0.027213"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saparating the data to train+test to have a supervised testset as well (10% of the train dataset)\n",
    "split_X_train, split_X_test, y_train, y_test = train_test_split(train_lines['text_combined'], train_lines['emotion'], test_size=0.2)\n",
    "\n",
    "#make sure the ratios are okay with the smaller test size, so the data can be learned well across all emotions.\n",
    "count = pd.DataFrame({\"train\": y_train.value_counts(), \"test\": y_test.value_counts()})\n",
    "count[\"ratio\"] = count.apply(lambda x: x['test']/x['train'], axis=1) #ideally: all ratios are of the same area\n",
    "count[\"train_ratio\"] = count.apply(lambda x: x['train']/len(y_train), axis=1)\n",
    "count[\"test_ratio\"] = count.apply(lambda x: x['test']/len(y_test), axis=1) # ideally: train ratio and test ratio are about the same\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b58ac6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T10:49:24.404676Z",
     "iopub.status.busy": "2023-12-29T10:49:24.403853Z",
     "iopub.status.idle": "2023-12-29T10:59:59.972827Z",
     "shell.execute_reply": "2023-12-29T10:59:59.971262Z"
    },
    "papermill": {
     "duration": 635.590546,
     "end_time": "2023-12-29T10:59:59.983964",
     "exception": false,
     "start_time": "2023-12-29T10:49:24.393418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_1500 creation time: 635.5589420795441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1164450x1500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11637701 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# build BOW with quite large size (to support as many keywords as possible) and remove all stopwords, then apply it it to trainset\n",
    "BOW_1500 = CountVectorizer(max_features=1500, stop_words='english', tokenizer=nltk.word_tokenize) \n",
    "\n",
    "BOW_1500.fit(split_X_train)\n",
    "\n",
    "X_train = BOW_1500.transform(split_X_train)\n",
    "X_test = BOW_1500.transform(split_X_test)\n",
    "\n",
    "print(\"BOW_1500 creation time:\", time.time() - start_time)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c506feb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T11:00:00.092575Z",
     "iopub.status.busy": "2023-12-29T11:00:00.091975Z",
     "iopub.status.idle": "2023-12-29T12:17:16.076881Z",
     "shell.execute_reply": "2023-12-29T12:17:16.075717Z"
    },
    "papermill": {
     "duration": 4637.179436,
     "end_time": "2023-12-29T12:17:17.260191",
     "exception": false,
     "start_time": "2023-12-29T11:00:00.080755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  CLASSES:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "input_shape:  1500 ; output_shape:  8\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1500)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               192128    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209672 (819.03 KB)\n",
      "Trainable params: 209672 (819.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Keras model creation time: 0.6869204044342041\n",
      "Epoch 1/25\n",
      "36390/36390 [==============================] - 160s 4ms/step - loss: 1.3801 - accuracy: 0.4963 - val_loss: 1.3468 - val_accuracy: 0.5081\n",
      "Epoch 2/25\n",
      "36390/36390 [==============================] - 157s 4ms/step - loss: 1.3212 - accuracy: 0.5172 - val_loss: 1.3333 - val_accuracy: 0.5139\n",
      "Epoch 3/25\n",
      "36390/36390 [==============================] - 159s 4ms/step - loss: 1.2970 - accuracy: 0.5271 - val_loss: 1.3316 - val_accuracy: 0.5154\n",
      "Epoch 4/25\n",
      "36390/36390 [==============================] - 160s 4ms/step - loss: 1.2798 - accuracy: 0.5329 - val_loss: 1.3365 - val_accuracy: 0.5137\n",
      "Epoch 5/25\n",
      "36390/36390 [==============================] - 161s 4ms/step - loss: 1.2662 - accuracy: 0.5387 - val_loss: 1.3394 - val_accuracy: 0.5154\n",
      "Epoch 6/25\n",
      "36390/36390 [==============================] - 165s 4ms/step - loss: 1.2554 - accuracy: 0.5424 - val_loss: 1.3445 - val_accuracy: 0.5142\n",
      "Epoch 7/25\n",
      "36390/36390 [==============================] - 161s 4ms/step - loss: 1.2467 - accuracy: 0.5462 - val_loss: 1.3549 - val_accuracy: 0.5123\n",
      "Epoch 8/25\n",
      "36390/36390 [==============================] - 163s 4ms/step - loss: 1.2391 - accuracy: 0.5490 - val_loss: 1.3549 - val_accuracy: 0.5118\n",
      "Epoch 9/25\n",
      "36390/36390 [==============================] - 162s 4ms/step - loss: 1.2320 - accuracy: 0.5516 - val_loss: 1.3617 - val_accuracy: 0.5113\n",
      "Epoch 10/25\n",
      "36390/36390 [==============================] - 158s 4ms/step - loss: 1.2257 - accuracy: 0.5542 - val_loss: 1.3680 - val_accuracy: 0.5102\n",
      "Epoch 11/25\n",
      "36390/36390 [==============================] - 162s 4ms/step - loss: 1.2202 - accuracy: 0.5566 - val_loss: 1.3742 - val_accuracy: 0.5086\n",
      "Epoch 12/25\n",
      "36390/36390 [==============================] - 157s 4ms/step - loss: 1.2151 - accuracy: 0.5587 - val_loss: 1.3793 - val_accuracy: 0.5085\n",
      "Epoch 13/25\n",
      "36390/36390 [==============================] - 152s 4ms/step - loss: 1.2107 - accuracy: 0.5604 - val_loss: 1.3883 - val_accuracy: 0.5076\n",
      "Epoch 14/25\n",
      "36390/36390 [==============================] - 160s 4ms/step - loss: 1.2065 - accuracy: 0.5623 - val_loss: 1.3902 - val_accuracy: 0.5057\n",
      "Epoch 15/25\n",
      "36390/36390 [==============================] - 161s 4ms/step - loss: 1.2026 - accuracy: 0.5636 - val_loss: 1.3968 - val_accuracy: 0.5062\n",
      "Epoch 16/25\n",
      "36390/36390 [==============================] - 172s 5ms/step - loss: 1.1992 - accuracy: 0.5654 - val_loss: 1.4056 - val_accuracy: 0.5069\n",
      "Epoch 17/25\n",
      "36390/36390 [==============================] - 164s 4ms/step - loss: 1.1954 - accuracy: 0.5671 - val_loss: 1.4085 - val_accuracy: 0.5051\n",
      "Epoch 18/25\n",
      "36390/36390 [==============================] - 175s 5ms/step - loss: 1.1927 - accuracy: 0.5678 - val_loss: 1.4189 - val_accuracy: 0.5036\n",
      "Epoch 19/25\n",
      "36390/36390 [==============================] - 168s 5ms/step - loss: 1.1900 - accuracy: 0.5688 - val_loss: 1.4153 - val_accuracy: 0.5026\n",
      "Epoch 20/25\n",
      "36390/36390 [==============================] - 165s 4ms/step - loss: 1.1875 - accuracy: 0.5697 - val_loss: 1.4280 - val_accuracy: 0.5029\n",
      "Epoch 21/25\n",
      "36390/36390 [==============================] - 162s 4ms/step - loss: 1.1852 - accuracy: 0.5709 - val_loss: 1.4283 - val_accuracy: 0.5011\n",
      "Epoch 22/25\n",
      "36390/36390 [==============================] - 165s 4ms/step - loss: 1.1830 - accuracy: 0.5719 - val_loss: 1.4342 - val_accuracy: 0.5017\n",
      "Epoch 23/25\n",
      "36390/36390 [==============================] - 169s 5ms/step - loss: 1.1806 - accuracy: 0.5728 - val_loss: 1.4464 - val_accuracy: 0.5010\n",
      "Epoch 24/25\n",
      "36390/36390 [==============================] - 169s 5ms/step - loss: 1.1787 - accuracy: 0.5731 - val_loss: 1.4470 - val_accuracy: 0.5019\n",
      "Epoch 25/25\n",
      "36390/36390 [==============================] - 170s 5ms/step - loss: 1.1771 - accuracy: 0.5741 - val_loss: 1.4493 - val_accuracy: 0.4992\n",
      "training finish\n",
      "2275/2275 [==============================] - 6s 3ms/step\n",
      "keras testing accuracy: 0.5\n",
      "Keras model fit time (from start): 4635.964226007462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'joy', 'joy', ..., 'joy', 'joy', 'joy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Keras model (+encoder and decoder) similarily to how it was made in the lab, with new layersizes- we now have 8 output options and the folloeing layers are size 8*4 and 8*8 accordingly\n",
    "start_time = time.time()\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train_enc = label_encode(label_encoder, y_train)\n",
    "y_test_enc = label_encode(label_encoder, y_test)\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print(len(label_encoder.classes_), \" CLASSES: \", label_encoder.classes_)\n",
    "print(\"input_shape: \", input_shape, \"; output_shape: \", output_shape)\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=128)(X)  # 128\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=32)(H1)  # 32\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "logs_file = LOGS_FILE\n",
    "\n",
    "csv_logger = CSVLogger(logs_file)\n",
    "print(\"Keras model creation time:\", time.time() - start_time)\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train_enc, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test_enc))\n",
    "print('training finish')\n",
    "\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(logs_file)\n",
    "\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "\n",
    "print('keras testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test_enc), pred_result), 2)))\n",
    "print(\"Keras model fit time (from start):\", time.time() - start_time)\n",
    "pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac15772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:17:45.888559Z",
     "iopub.status.busy": "2023-12-29T12:17:45.888073Z",
     "iopub.status.idle": "2023-12-29T12:17:48.539038Z",
     "shell.execute_reply": "2023-12-29T12:17:48.537887Z"
    },
    "papermill": {
     "duration": 9.892109,
     "end_time": "2023-12-29T12:17:48.542086",
     "exception": false,
     "start_time": "2023-12-29T12:17:38.649977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save results summary into json\n",
    "res_summary = {\n",
    "    \"keras\":{\n",
    "        \"test\": format(round(accuracy_score(label_decode(label_encoder, y_test_enc), pred_result), 2))\n",
    "    }\n",
    "}\n",
    "with open(\"summary.json\", \"w\") as outfile:\n",
    "    json.dump(res_summary, outfile)\n",
    "\n",
    "# save all supervised test results into 1 df and export as CSV\n",
    "# all_test_responses = pd.DataFrame({\"id\": test_sup['tweet_id'], \"DT\": y_test_pred, \"mnb\": y_test_pred_mnb, \"ker\": pred_result})\n",
    "all_test_responses = pd.DataFrame({\"id\": split_X_test, \"emotion\": pred_result})\n",
    "all_test_responses.to_csv('test_sup_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b396e74d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:18:02.798250Z",
     "iopub.status.busy": "2023-12-29T12:18:02.797838Z",
     "iopub.status.idle": "2023-12-29T12:20:08.070834Z",
     "shell.execute_reply": "2023-12-29T12:20:08.069543Z"
    },
    "papermill": {
     "duration": 132.462351,
     "end_time": "2023-12-29T12:20:08.073818",
     "exception": false,
     "start_time": "2023-12-29T12:17:55.611467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3219/3219 [==============================] - 10s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# run against real test dataset (and not the supervised one):\n",
    "X_test_real = BOW_1500.transform(test_lines['text_combined'])\n",
    "y_test_pred_ker_enc = model.predict(X_test_real, batch_size=128)\n",
    "y_test_pred_ker = label_decode(label_encoder, y_test_pred_ker_enc)\n",
    "\n",
    "#save submission file\n",
    "submit_responses_mnb = pd.DataFrame({\"id\": test_lines['tweet_id'], \"emotion\": y_test_pred_ker})\n",
    "submit_responses_mnb.set_index(\"id\").to_csv('submission_ker.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6968932,
     "sourceId": 63751,
     "sourceType": "competition"
    },
    {
     "datasetId": 4216985,
     "sourceId": 7273959,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6118.878686,
   "end_time": "2023-12-29T12:20:18.502884",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-29T10:38:19.624198",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
